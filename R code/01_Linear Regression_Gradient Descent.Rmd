---
title: "Linear Regression"
output: html_document
---

```{r setup, include=T}
knitr::opts_chunk$set(echo = T, include = T, wwarning = F)
```

## Boston data

Boston is a database with information of areas around Boston city, and the median house prices. It is included in the MASS R package.
The essential data manipulation will be performed with the tidyverse collection of R packages.

```{r}
library(MASS)
library(tidyverse)
```

We will use linear regression to infer the relationship between the percentage of lower status of the population (lstat) and house prices (medv).

## First linear regression model

Fit of the basic linear regression model using lm function. 

```{r}
fit1 <- lm(medv ~ lstat, data = Boston)
summary(fit1)
```

##	Gradient descent


```{r}
# squared error cost function
cost <- function(X, y, theta) {
  sum( (X %*% theta - y)^2 ) / (2*length(y))
}

# learning rate and iteration limit
alpha <- 0.001
num_iters <- 100

# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)

# initialize coefficients
theta <- matrix(c(0,0), nrow=2)

# add a column of 1's for the intercept coefficient
X <- cbind(1, matrix(Boston$lstat))
#X <- model.matrix(fit1)
y <- Boston$medv

# gradient descent
gradientDesc <- function(x, y, learn_rate, conv_threshold, n, max_iter) {
  plot(x, y, col = "blue", pch = 20)
  m <- runif(1, 0, 1)
  c <- runif(1, 0, 1)
  yhat <- m * x + c
  MSE <- sum((y - yhat) ^ 2) / n
  converged = F
  iterations = 0
  while(converged == F) {
    ## Implement the gradient descent algorithm
    m_new <- m - learn_rate * ((1 / n) * (sum((yhat - y) * x)))
    c_new <- c - learn_rate * ((1 / n) * (sum(yhat - y)))
    m <- m_new
    c <- c_new
    yhat <- m * x + c
    MSE_new <- sum((y - yhat) ^ 2) / n
    if(MSE - MSE_new <= conv_threshold) {
      abline(c, m) 
      converged = T
      return(paste("Optimal intercept:", c, "Optimal slope:", m))
    }
    iterations = iterations + 1
    if(iterations > max_iter) { 
      abline(c, m) 
      converged = T
      return(paste("Optimal intercept:", c, "Optimal slope:", m))
    }
  }
}


# Run the function 

gradientDesc(Boston$lstat, Boston$medv, 0.0000293, 0.001, 32, 2500000)



```



## Resources
https://www.r-bloggers.com/2017/02/implementing-the-gradient-descent-algorithm-in-r/